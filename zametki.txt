1. Генерация данных с помощью sample()
def sample():
    x = np.linspace(0, 2 * math.pi, 1000)  # Массив из 1000 точек от 0 до 2π
    выглядит как Array из 100 элементов от 0.0 до 0.6226580034141932
    y = 100 * np.sin(x) + 0.5 * np.exp(x) + 300 + 10 * np.random.randn(1000)
    выглядит как Array из 100 элементов от ~292.02440147520934 до ~345.0651607107256
    return x, y  # [], []
2. Наша модель со степенями не меняется, меняется обучаемая кривая с каждой степенью обучаемости
3. Кривая стоится по типу Растущего полинома? - да
4. Вид модели:
5. Будем учить через метод наи квадратов
6. Нужно найти t (target) = y(x,w) где x полиномиальная фнкц, а w - обучаемые параметры
7. Чтобы было не тупо по точкам, нужен шум в виде нормального распределения (-ε;ε)
8. t = y(x,w) + ε
9. Как описать обучаемую полиномиальную кривую? да
Исп онли numpy и matplotlib

Функция polynomial_fit_and_plot()
- будет обучать модель полином регрессии 20ю эпохами ( степенями)

fig, axes = plt.subplots(4, 5, figsize=(18, 12))  - стека 4 на 5 пустых подграфиков
axes.flatten() - обращаться к каждому графику как к элементу массива

sse_values = []  Для сохранения ошибок (SSE)

Делаем цикл по эпохам
for degree in range(1, max_degree + 1):
здесь для каждой эпохи(степени)
вычисляется модель, предсказания, ошибка и всохр в визуализейшн результат

Про матрицу признаков:
X = np.vander(x, degree + 1, increasing=True) ( vander для веторов степеней)

Про обучение весов
weights = np.linalg.pinv(X) @ y
Х - матрица: столбцы == степень полинома
pinv(X) - делаем псевдообратную матрицу Х,
т.е Для нахождения весов в задаче регрессии используется метод
наименьших квадратов: w = (X(транпон)* Х)^(-1) *Х(транспон) *y
то уравнение позволяет найти веса w,
которые минимизируют ошибку между предсказанными и фактическими значениями

ПОэтому здесь используется вместо явного вычисления X(транпон)* Х)^(-1) *Х(транспон) *y
псевдообразная матрциа pinv(X)   (((мы хотим минимизировать сумму квадратов ошибок (SSE)
поэтому нам нужно вычислить производную ошибки по w и приравнять потом это к нулю,
но если X(транспон)*Х окажется вырожденной(т.е лин зависимой), то обратной матрицы может не сущетсвовать
Что застопит нам решение. Поэтому Псевдообратная матрица решит это, вычислив её(она всегда сущ)


pinv(X) @ y - это операция умножения даёт вектор весов w, где w содержит коэффы подинома

w=[w0,w1,w2,w3..,wd]


Далее вычисляется ПРЕДИКШН
y_pred = X @ weights   (== y pred = X * w == w0 +w1*xi+w2*xi^2...+wd*xi^d)
результатом будет массив пердсказанных y-ков


Далее вычисляеются ошибки SSE
сумму квадратов ошибок для КАЖДОЙ степени полинома
ошибка = yi - y pred.i
и ошибка^2 = (yi - y pred.i )^2
+ сохран в sse_values


Визуализация по каждой эпохе

ax = axes[degree - 1]
ax.scatter(x, y, color='red', s=1, label='Data')  (scatter = Точки данных)
ax.plot(x, y_pred, color='blue', label='Fit') (полином. линия)
ax.set_title(f"deg={degree}")
ax.set_ylim([y.min() - 50, y.max() + 50]) Устанавливает границы графика


График ошибок SSE
plt.figure(figsize=(8, 5))
plt.plot(range(1, max_degree + 1), sse_values, color='red', marker='o', label='SSE')
plt.title("Error SSE calculation")
plt.xlabel("Polynomial Degree")
plt.ylabel("SSE")
plt.grid()
plt.legend()
plt.savefig("sse_plot.png")
plt.close()
